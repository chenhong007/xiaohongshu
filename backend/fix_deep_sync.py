#!/usr/bin/env python3
"""
æ·±åº¦åŒæ­¥ä¿®å¤è„šæœ¬

åŠŸèƒ½ï¼š
1. æ£€æŸ¥æ•°æ®åº“ä¸­æ‰€æœ‰ç¼ºå¤±æ·±åº¦å­—æ®µï¼ˆupload_time, collected_count ç­‰ï¼‰çš„ç¬”è®°
2. ä»¥æ›´æ…¢çš„é€Ÿåº¦ï¼ˆ5-15ç§’é—´éš”ï¼‰é‡æ–°è·å–è¯¦æƒ…ï¼Œé¿å…è¢«é™æµ
3. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¯éšæ—¶ä¸­æ–­åç»§ç»­
4. è¯Šæ–­é™æµçŠ¶æ€

ä½¿ç”¨æ–¹æ³•ï¼š
    # åœ¨ Docker å®¹å™¨å†…è¿è¡Œ
    docker exec -it xhs-backend python fix_deep_sync.py --check        # ä»…æ£€æŸ¥ç¼ºå¤±æ•°æ®
    docker exec -it xhs-backend python fix_deep_sync.py --diagnose     # è¯Šæ–­é™æµçŠ¶æ€
    docker exec -it xhs-backend python fix_deep_sync.py --fix          # ä¿®å¤ç¼ºå¤±æ•°æ®
    docker exec -it xhs-backend python fix_deep_sync.py --fix --user-id 59757acd50c4b45e6e9a90df  # åªä¿®å¤æŒ‡å®šç”¨æˆ·
    docker exec -it xhs-backend python fix_deep_sync.py --fix --limit 50   # é™åˆ¶ä¿®å¤æ•°é‡
    
âš ï¸ æ³¨æ„ï¼šå¦‚æœè¢«é™æµï¼Œéœ€è¦ç­‰å¾…å‡ å°æ—¶ç”šè‡³æ›´é•¿æ—¶é—´æ‰èƒ½æ¢å¤ï¼
   å»ºè®®ï¼šæ›´æ¢ Cookieï¼ˆé‡æ–°ç™»å½•ï¼‰æˆ–ç­‰å¾…é™æµè§£é™¤åå†å°è¯•
"""

import os
import sys
import time
import json
import random
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app import create_app
from app.extensions import db
from app.models import Note, Account, Cookie
from app.config import Config

# å…¨å±€é…ç½®
MIN_DELAY = 5.0       # æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
MAX_DELAY = 15.0      # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
EXTRA_PAUSE_CHANCE = 0.2   # é¢å¤–é•¿æš‚åœæ¦‚ç‡
EXTRA_PAUSE_MAX = 30.0     # é¢å¤–é•¿æš‚åœæœ€å¤§ç§’æ•°
MAX_RETRIES = 5       # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY_BASE = 10 # é‡è¯•åŸºç¡€å»¶è¿Ÿ


def get_missing_notes_query(user_id=None):
    """æ„å»ºç¼ºå¤±æ·±åº¦æ•°æ®çš„ç¬”è®°æŸ¥è¯¢"""
    # ç¼ºå¤±çš„æ¡ä»¶ï¼šupload_time ä¸ºç©º æˆ– collected_count/comment_count/share_count éƒ½ä¸º0
    missing_condition = db.or_(
        Note.upload_time.is_(None),
        Note.upload_time == '',
        db.and_(
            Note.collected_count == 0,
            Note.comment_count == 0,
            Note.share_count == 0
        )
    )
    
    query = Note.query.filter(missing_condition)
    
    if user_id:
        query = query.filter(Note.user_id == user_id)
    
    return query


def check_missing_data(user_id=None):
    """æ£€æŸ¥ç¼ºå¤±æ·±åº¦æ•°æ®çš„ç¬”è®°"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š")
    print("=" * 60)
    
    # æ€»ç¬”è®°æ•°
    total_notes = Note.query.count()
    print(f"\nğŸ“ æ€»ç¬”è®°æ•°: {total_notes}")
    
    # ç¼ºå¤± upload_time
    missing_upload_time = Note.query.filter(
        db.or_(Note.upload_time.is_(None), Note.upload_time == '')
    ).count()
    print(f"âš ï¸  ç¼ºå¤±å‘å¸ƒæ—¶é—´ (upload_time): {missing_upload_time}")
    
    # ç¼ºå¤±äº’åŠ¨æ•°æ®ï¼ˆä¸‰é¡¹éƒ½ä¸º0ï¼‰
    missing_interaction = Note.query.filter(
        Note.collected_count == 0,
        Note.comment_count == 0,
        Note.share_count == 0
    ).count()
    print(f"âš ï¸  ç¼ºå¤±äº’åŠ¨æ•°æ® (collected/comment/share éƒ½ä¸º0): {missing_interaction}")
    
    # ç¼ºå¤±è§†é¢‘åœ°å€ï¼ˆè§†é¢‘ç±»å‹ï¼‰
    missing_video = Note.query.filter(
        Note.type == 'è§†é¢‘',
        db.or_(Note.video_addr.is_(None), Note.video_addr == '')
    ).count()
    print(f"âš ï¸  è§†é¢‘ç¬”è®°ç¼ºå¤±è§†é¢‘åœ°å€: {missing_video}")
    
    # ç¼ºå¤±æœ¬åœ°å°é¢
    missing_cover = Note.query.filter(
        db.or_(Note.cover_local.is_(None), Note.cover_local == '')
    ).count()
    print(f"âš ï¸  ç¼ºå¤±æœ¬åœ°å°é¢: {missing_cover}")
    
    # ç»¼åˆï¼šéœ€è¦æ·±åº¦åŒæ­¥çš„ç¬”è®°
    need_fix = get_missing_notes_query(user_id).count()
    print(f"\nğŸ”§ éœ€è¦ä¿®å¤çš„ç¬”è®°æ€»æ•°: {need_fix}")
    
    # æŒ‰ç”¨æˆ·ç»Ÿè®¡
    print("\nğŸ“Š æŒ‰ç”¨æˆ·ç»Ÿè®¡ç¼ºå¤±æ•°æ®:")
    print("-" * 50)
    
    user_stats = db.session.query(
        Note.user_id,
        Note.nickname,
        db.func.count(Note.note_id).label('missing_count')
    ).filter(
        db.or_(Note.upload_time.is_(None), Note.upload_time == '')
    ).group_by(Note.user_id).order_by(db.desc('missing_count')).all()
    
    for i, (uid, nickname, count) in enumerate(user_stats[:20], 1):
        print(f"  {i:2d}. {nickname or uid[:16]}: {count} æ¡")
    
    if len(user_stats) > 20:
        print(f"  ... è¿˜æœ‰ {len(user_stats) - 20} ä¸ªç”¨æˆ·")
    
    print("\n" + "=" * 60)
    return need_fix


def get_cookie_str():
    """è·å–æœ‰æ•ˆçš„ Cookie å­—ç¬¦ä¸²"""
    cookie = Cookie.query.filter_by(is_active=True, is_valid=True).first()
    if cookie:
        return cookie.get_cookie_str()
    return None


def diagnose_rate_limit():
    """è¯Šæ–­å½“å‰çš„é™æµçŠ¶æ€"""
    print("\n" + "=" * 60)
    print("ğŸ” é™æµçŠ¶æ€è¯Šæ–­")
    print("=" * 60)
    
    # æ£€æŸ¥ Cookie
    cookie_str = get_cookie_str()
    if not cookie_str:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ Cookieï¼Œè¯·å…ˆç™»å½•")
        return
    print("âœ… Cookie çŠ¶æ€æ­£å¸¸")
    
    # æ£€æŸ¥ Cookie è¯¦æƒ…
    cookie = Cookie.query.filter_by(is_active=True, is_valid=True).first()
    if cookie:
        print(f"   ç”¨æˆ·: {cookie.nickname}")
        print(f"   æœ€åæ£€æŸ¥: {cookie.last_checked}")
    
    # åˆå§‹åŒ– API
    try:
        from apis.xhs_pc_apis import XHS_Apis
        xhs_apis = XHS_Apis()
        print("âœ… API åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ API åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # å°è¯•è·å–ä¸€ä¸ªæœ‰è¯¦æƒ…çš„ç¬”è®°ï¼ˆå·²çŸ¥æˆåŠŸçš„ï¼‰
    test_note_id = "693804f3000000000d03c410"  # å·²çŸ¥æœ‰è¯¦æƒ…çš„ç¬”è®°
    test_url = f"https://www.xiaohongshu.com/explore/{test_note_id}"
    
    print(f"\nğŸ§ª æµ‹è¯•è¯·æ±‚ï¼ˆç¬”è®° ID: {test_note_id}ï¼‰...")
    
    success, msg, result = xhs_apis.get_note_info(test_url, cookie_str)
    
    if 'é¢‘æ¬¡å¼‚å¸¸' in str(msg) or 'é¢‘ç¹æ“ä½œ' in str(msg):
        print(f"âŒ å½“å‰å¤„äºé™æµçŠ¶æ€: {msg}")
        print("\nğŸ’¡ å»ºè®®:")
        print("   1. ç­‰å¾… 2-6 å°æ—¶åå†è¯•")
        print("   2. æ›´æ¢ Cookieï¼ˆé‡æ–°ç™»å½•å¦ä¸€ä¸ªè´¦å·ï¼‰")
        print("   3. å¦‚æœä½¿ç”¨ä»£ç†ï¼Œå°è¯•æ›´æ¢ IP")
    elif not success:
        print(f"âš ï¸  è¯·æ±‚å¤±è´¥: {msg}")
        if 'ç™»å½•' in str(msg) or 'unauthorized' in str(msg).lower():
            print("   å¯èƒ½æ˜¯ Cookie å·²è¿‡æœŸï¼Œè¯·é‡æ–°ç™»å½•")
    else:
        print("âœ… è¯·æ±‚æˆåŠŸï¼Œå½“å‰æœªè¢«é™æµï¼")
        if result and result.get('data', {}).get('items'):
            print("   å¯ä»¥æ­£å¸¸è·å–è¯¦æƒ…æ•°æ®")
    
    # æ£€æŸ¥è´¦å·çš„ xsec_token
    print("\nğŸ“Š è´¦å· xsec_token çŠ¶æ€:")
    accounts = Account.query.all()
    with_token = sum(1 for a in accounts if a.xsec_token)
    print(f"   æœ‰ token: {with_token}/{len(accounts)} ä¸ªè´¦å·")
    
    print("\n" + "=" * 60)


def sleep_with_jitter():
    """å¸¦éšæœºæŠ–åŠ¨çš„å»¶è¿Ÿ"""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    
    # å¶å°”å¢åŠ é¢å¤–é•¿æš‚åœ
    if random.random() < EXTRA_PAUSE_CHANCE:
        extra = random.uniform(5.0, EXTRA_PAUSE_MAX)
        delay += extra
        print(f"  ğŸ’¤ é¢å¤–æš‚åœ {extra:.1f}s...")
    
    time.sleep(delay)


def fix_note_detail(note, xhs_apis, cookie_str, account_xsec_token):
    """ä¿®å¤å•ä¸ªç¬”è®°çš„è¯¦æƒ…æ•°æ®"""
    from xhs_utils.data_util import handle_note_info
    
    note_id = note.note_id
    
    # æ„å»ºç¬”è®°URL
    xsec_token = note.xsec_token or account_xsec_token
    if xsec_token:
        note_url = f"https://www.xiaohongshu.com/explore/{note_id}?xsec_token={xsec_token}&xsec_source=pc_search"
    else:
        note_url = f"https://www.xiaohongshu.com/explore/{note_id}"
    
    # å¸¦é‡è¯•çš„è¯¦æƒ…è·å–
    for attempt in range(MAX_RETRIES):
        if attempt > 0:
            wait_time = RETRY_DELAY_BASE * (attempt + 1) + random.uniform(5, 15)
            print(f"    â³ é‡è¯• {attempt + 1}/{MAX_RETRIES}ï¼Œç­‰å¾… {wait_time:.1f}s...")
            time.sleep(wait_time)
        
        success, msg, note_detail = xhs_apis.get_note_info(note_url, cookie_str)
        
        # æ£€æŸ¥é™æµ
        if 'é¢‘æ¬¡å¼‚å¸¸' in str(msg) or 'é¢‘ç¹æ“ä½œ' in str(msg):
            print(f"    âš ï¸  è¢«é™æµ: {msg}")
            if attempt < MAX_RETRIES - 1:
                continue
            else:
                print(f"    âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡")
                return False
        
        # æ£€æŸ¥ç¬”è®°ä¸å¯ç”¨
        if 'æš‚æ—¶æ— æ³•æµè§ˆ' in str(msg) or 'ç¬”è®°ä¸å­˜åœ¨' in str(msg):
            print(f"    âš ï¸  ç¬”è®°ä¸å¯ç”¨: {msg}")
            return False
        
        if not success:
            print(f"    âš ï¸  è·å–å¤±è´¥: {msg}")
            if attempt < MAX_RETRIES - 1:
                continue
            return False
        
        if success and note_detail:
            try:
                data = note_detail.get('data')
                if data and data.get('items') and len(data['items']) > 0:
                    note_data = data['items'][0]
                    note_data['url'] = note_url
                    cleaned_data = handle_note_info(note_data, from_list=False, xsec_token=xsec_token)
                    
                    # æ›´æ–°ç¬”è®°æ•°æ®
                    if cleaned_data['upload_time']:
                        note.upload_time = cleaned_data['upload_time']
                    if cleaned_data['collected_count'] is not None:
                        note.collected_count = cleaned_data['collected_count']
                    if cleaned_data['comment_count'] is not None:
                        note.comment_count = cleaned_data['comment_count']
                    if cleaned_data['share_count'] is not None:
                        note.share_count = cleaned_data['share_count']
                    if cleaned_data['video_addr']:
                        note.video_addr = cleaned_data['video_addr']
                    if cleaned_data['desc']:
                        note.desc = cleaned_data['desc']
                    if cleaned_data['image_list']:
                        new_count = len(cleaned_data['image_list'])
                        try:
                            old_list = json.loads(note.image_list) if note.image_list else []
                            old_count = len(old_list)
                        except:
                            old_count = 0
                        if new_count > old_count:
                            note.image_list = json.dumps(cleaned_data['image_list'])
                    if cleaned_data['tags']:
                        note.tags = json.dumps(cleaned_data['tags'])
                    if cleaned_data['ip_location']:
                        note.ip_location = cleaned_data['ip_location']
                    
                    note.last_updated = datetime.utcnow()
                    db.session.commit()
                    
                    return True
                else:
                    print(f"    âš ï¸  å“åº”æ•°æ®ä¸ºç©º")
                    if attempt < MAX_RETRIES - 1:
                        continue
                    return False
            except Exception as e:
                print(f"    âŒ è§£æé”™è¯¯: {e}")
                return False
    
    return False


def fix_missing_data(user_id=None, limit=None, dry_run=False):
    """ä¿®å¤ç¼ºå¤±çš„æ·±åº¦æ•°æ®"""
    print("\n" + "=" * 60)
    print("ğŸ”§ å¼€å§‹ä¿®å¤ç¼ºå¤±çš„æ·±åº¦æ•°æ®")
    print("=" * 60)
    
    if dry_run:
        print("âš ï¸  DRY RUN æ¨¡å¼ï¼Œä¸ä¼šå®é™…ä¿®æ”¹æ•°æ®")
    
    # è·å– Cookie
    cookie_str = get_cookie_str()
    if not cookie_str:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ Cookieï¼Œè¯·å…ˆç™»å½•")
        return
    
    print("âœ… Cookie æœ‰æ•ˆ")
    
    # åˆå§‹åŒ– API
    try:
        from apis.xhs_pc_apis import XHS_Apis
        xhs_apis = XHS_Apis()
        print("âœ… API åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ API åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # è·å–éœ€è¦ä¿®å¤çš„ç¬”è®°
    query = get_missing_notes_query(user_id)
    if limit:
        query = query.limit(limit)
    
    notes = query.all()
    total = len(notes)
    
    if total == 0:
        print("âœ… æ²¡æœ‰éœ€è¦ä¿®å¤çš„ç¬”è®°")
        return
    
    print(f"\nğŸ“ å¾…ä¿®å¤ç¬”è®°: {total} æ¡")
    print(f"â±ï¸  é¢„è®¡è€—æ—¶: {total * (MIN_DELAY + MAX_DELAY) / 2 / 60:.1f} - {total * MAX_DELAY / 60:.1f} åˆ†é’Ÿ")
    print(f"âš™ï¸  å»¶è¿Ÿé…ç½®: {MIN_DELAY}-{MAX_DELAY}ç§’ï¼Œé¢å¤–æš‚åœæ¦‚ç‡ {EXTRA_PAUSE_CHANCE*100}%")
    
    if not dry_run:
        print("\næŒ‰ Ctrl+C å¯éšæ—¶ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
        time.sleep(3)
    
    # è·å–è´¦å·çš„ xsec_token æ˜ å°„
    account_tokens = {}
    accounts = Account.query.all()
    for acc in accounts:
        if acc.xsec_token:
            account_tokens[acc.user_id] = acc.xsec_token
    
    # å¼€å§‹ä¿®å¤
    success_count = 0
    fail_count = 0
    skip_count = 0
    
    print("\n" + "-" * 60)
    
    try:
        for i, note in enumerate(notes, 1):
            note_id = note.note_id
            title = (note.title or '')[:30]
            nickname = note.nickname or note.user_id[:10]
            
            print(f"\n[{i}/{total}] {nickname} - {title}...")
            
            if dry_run:
                print(f"  ğŸ“‹ DRY RUN: å°†ä¿®å¤ note_id={note_id}")
                skip_count += 1
                continue
            
            # è·å–è´¦å·çš„ xsec_token
            account_xsec_token = account_tokens.get(note.user_id, '')
            
            # ä¿®å¤ç¬”è®°è¯¦æƒ…
            if fix_note_detail(note, xhs_apis, cookie_str, account_xsec_token):
                success_count += 1
                print(f"  âœ… æˆåŠŸï¼upload_time={note.upload_time}")
            else:
                fail_count += 1
                print(f"  âŒ å¤±è´¥")
            
            # å»¶è¿Ÿ
            if i < total:
                print(f"  ğŸ’¤ ç­‰å¾…ä¸­...")
                sleep_with_jitter()
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        db.session.commit()
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¿®å¤ç»“æœ")
    print("=" * 60)
    print(f"  âœ… æˆåŠŸ: {success_count}")
    print(f"  âŒ å¤±è´¥: {fail_count}")
    if dry_run:
        print(f"  ğŸ“‹ è·³è¿‡ (dry run): {skip_count}")
    print("=" * 60)


def main():
    global MIN_DELAY, MAX_DELAY
    
    parser = argparse.ArgumentParser(description='æ·±åº¦åŒæ­¥ä¿®å¤è„šæœ¬')
    parser.add_argument('--check', action='store_true', help='ä»…æ£€æŸ¥ç¼ºå¤±æ•°æ®')
    parser.add_argument('--diagnose', action='store_true', help='è¯Šæ–­é™æµçŠ¶æ€')
    parser.add_argument('--fix', action='store_true', help='ä¿®å¤ç¼ºå¤±æ•°æ®')
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…ä¿®æ”¹')
    parser.add_argument('--user-id', type=str, help='åªå¤„ç†æŒ‡å®šç”¨æˆ·ID')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†æ•°é‡')
    parser.add_argument('--min-delay', type=float, default=MIN_DELAY, help=f'æœ€å°å»¶è¿Ÿç§’æ•° (é»˜è®¤ {MIN_DELAY})')
    parser.add_argument('--max-delay', type=float, default=MAX_DELAY, help=f'æœ€å¤§å»¶è¿Ÿç§’æ•° (é»˜è®¤ {MAX_DELAY})')
    
    args = parser.parse_args()
    
    # æ›´æ–°å»¶è¿Ÿé…ç½®
    MIN_DELAY = args.min_delay
    MAX_DELAY = args.max_delay
    
    # åˆ›å»º Flask åº”ç”¨ä¸Šä¸‹æ–‡
    app = create_app()
    
    with app.app_context():
        if args.diagnose:
            diagnose_rate_limit()
        elif args.check:
            check_missing_data(args.user_id)
        elif args.fix:
            fix_missing_data(args.user_id, args.limit, args.dry_run)
        else:
            # é»˜è®¤å…ˆæ£€æŸ¥
            need_fix = check_missing_data(args.user_id)
            if need_fix > 0:
                print("\nğŸ’¡ æç¤º:")
                print("   1. ä½¿ç”¨ --diagnose è¯Šæ–­é™æµçŠ¶æ€")
                print("   2. ä½¿ç”¨ --fix å‚æ•°å¼€å§‹ä¿®å¤")
                print("   ä¾‹å¦‚: python fix_deep_sync.py --diagnose")
                print("   ä¾‹å¦‚: python fix_deep_sync.py --fix --limit 50")


if __name__ == '__main__':
    main()

